{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tomli in /Users/golobs/Library/Caches/pypoetry/virtualenvs/midst-models-Xt6qy6Zf-py3.9/lib/python3.9/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tomli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Berka Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithm]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T20:55:45.248820Z",
     "start_time": "2024-12-27T20:55:43.840945Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tomli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_data\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess, TabularDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabsyn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabSyn\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_config\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_threads(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m install()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/data.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_metrics \u001b[38;5;28;01mas\u001b[39;00m calculate_metrics_\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaskType, dump_pickle, get_categories, load_json, load_pickle\n\u001b[1;32m     21\u001b[0m ArrayDict \u001b[38;5;241m=\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/metrics.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mskm\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaskType, raise_unknown\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPredictionType\u001b[39;00m(enum\u001b[38;5;241m.\u001b[39mEnum):\n\u001b[1;32m     12\u001b[0m     LOGITS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/util.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Union, cast\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtomli\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtomli_w\u001b[39;00m\n\u001b[1;32m     10\u001b[0m RawConfig \u001b[38;5;241m=\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tomli'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import preprocess, TabularDataset\n",
    "from src.tabsyn.pipeline import TabSyn\n",
    "from src import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Berka Dataset\n",
    "\n",
    "In this section, we will process the Transactions table from the Berka dataset. You can access the Berka dataset files for TabSyn [here](https://drive.google.com/drive/folders/1rmJ_E6IzG25eCL3foYAb2jVmAstXktJ1?usp=drive_link). The BERKA dataset is a comprehensive banking dataset originally released by the Czech bank ČSOB for the Financial Modeling and Analysis (FMA) competition in 1999. It provides detailed financial data on transactions, accounts, loans, credit cards, and demographic information for thousands of customers over multiple years.\n",
    "\n",
    "Download the data files from the link above and place the train set in the `RAW_DATA_DIR` directory.\n",
    "Note that the id columns (columns ending in \"_id\") should be removed from the training and test data.\n",
    "\n",
    "Data info files are required for running the scripts. Sample info file for the transaction data is available in `data_info/trans.json`. The paths for the training and test data in the file can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:47:49.657048Z",
     "start_time": "2024-12-26T20:47:49.654661Z"
    }
   },
   "outputs": [],
   "source": [
    "INFO_DIR = \"data_info\"\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"trans\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:47:51.221065Z",
     "start_time": "2024-12-26T20:47:51.203008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_id</th>\n",
       "      <th>account_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>trans_type</th>\n",
       "      <th>operation</th>\n",
       "      <th>amount</th>\n",
       "      <th>balance</th>\n",
       "      <th>k_symbol</th>\n",
       "      <th>bank</th>\n",
       "      <th>account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330530</td>\n",
       "      <td>1126</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>20515.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50642</td>\n",
       "      <td>169</td>\n",
       "      <td>2129</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>65847.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>931992</td>\n",
       "      <td>3178</td>\n",
       "      <td>1641</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>13507.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1058060</td>\n",
       "      <td>3617</td>\n",
       "      <td>515</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>36742.7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>564471</td>\n",
       "      <td>1921</td>\n",
       "      <td>1984</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>16299.2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>78194778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>431367</td>\n",
       "      <td>1465</td>\n",
       "      <td>978</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>24297.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>68264</td>\n",
       "      <td>227</td>\n",
       "      <td>1519</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29486.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>636892</td>\n",
       "      <td>2175</td>\n",
       "      <td>333</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>36753.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>984504</td>\n",
       "      <td>3357</td>\n",
       "      <td>1368</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>26366.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>75957</td>\n",
       "      <td>253</td>\n",
       "      <td>1840</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>76840.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trans_id  account_id  trans_date  trans_type  operation  amount  \\\n",
       "0        330530        1126         336           0          3  2400.0   \n",
       "1         50642         169        2129           2          4    14.6   \n",
       "2        931992        3178        1641           2          4    14.6   \n",
       "3       1058060        3617         515           2          4    14.6   \n",
       "4        564471        1921        1984           0          2  3650.0   \n",
       "...         ...         ...         ...         ...        ...     ...   \n",
       "19995    431367        1465         978           2          4  1300.0   \n",
       "19996     68264         227        1519           2          4    14.6   \n",
       "19997    636892        2175         333           2          4    14.6   \n",
       "19998    984504        3357        1368           2          4    14.6   \n",
       "19999     75957         253        1840           2          5  3800.0   \n",
       "\n",
       "       balance  k_symbol  bank   account  \n",
       "0      20515.0         1     0         0  \n",
       "1      65847.0         6     0         0  \n",
       "2      13507.4         6     0         0  \n",
       "3      36742.7         6     0         0  \n",
       "4      16299.2         1     8  78194778  \n",
       "...        ...       ...   ...       ...  \n",
       "19995  24297.0         1     0         0  \n",
       "19996  29486.9         6     0         0  \n",
       "19997  36753.6         6     0         0  \n",
       "19998  26366.0         6     0         0  \n",
       "19999  76840.0         1     0         0  \n",
       "\n",
       "[20000 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process data\n",
    "# process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(\"../../data/tabsyn_white_box/train/tabsyn_1/train_with_id.csv\")\n",
    "df.head(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:47:52.851774Z",
     "start_time": "2024-12-26T20:47:52.847790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'trans',\n",
       " 'task_type': 'regression',\n",
       " 'header': 'infer',\n",
       " 'column_names': ['trans_date',\n",
       "  'trans_type',\n",
       "  'operation',\n",
       "  'amount',\n",
       "  'balance',\n",
       "  'k_symbol',\n",
       "  'bank',\n",
       "  'account'],\n",
       " 'num_col_idx': [0, 4, 7],\n",
       " 'cat_col_idx': [1, 2, 5, 6],\n",
       " 'target_col_idx': [3],\n",
       " 'file_type': 'csv',\n",
       " 'data_path': '/projects/aieng/midst_competition/data/tabsyn/tabsyn_1/raw_data/train.csv',\n",
       " 'test_path': '/projects/aieng/midst_competition/data/tabsyn/tabsyn_1/raw_data/test.csv'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review json file and its contents\n",
    "with open(\"data_info/trans.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that if you want to use a subset of the entire transaction table, you must still preprocess the full table, retain the main table, and pass it as the reference data to `preprocess` later. This is because the model should have access to all the categories for categorical columns in the data.\n",
    "\n",
    "The sample data info files is available in `data_info/trans_all.json`. The paths for the training and test data in the file can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:51:10.016352Z",
     "start_time": "2024-12-26T19:51:09.957568Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projects/aieng/midst_competition/data/tabsyn/all_data/raw_data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PROCESSED_DATA_DIR_ALL \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR_ALL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m DATA_NAME_ALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrans_all\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_NAME_ALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINFO_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR_ALL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m REF_DATA_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROCESSED_DATA_DIR_ALL, DATA_NAME_ALL)\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/scripts/process_dataset.py:140\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(name, info_path, data_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m data_path \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     data_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m     data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(data_path, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/projects/aieng/midst_competition/data/tabsyn/all_data/raw_data/train.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR_ALL = \"all_data/\"\n",
    "RAW_DATA_DIR_ALL = os.path.join(DATA_DIR_ALL, \"raw_data\")\n",
    "PROCESSED_DATA_DIR_ALL = os.path.join(DATA_DIR_ALL, \"processed_data\")\n",
    "DATA_NAME_ALL = \"trans_all\"\n",
    "\n",
    "process_data(DATA_NAME_ALL, INFO_DIR, DATA_DIR_ALL)\n",
    "\n",
    "REF_DATA_PATH = os.path.join(PROCESSED_DATA_DIR_ALL, DATA_NAME_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/a7e6a218-dd8e-4ae8-a8e5-6fc3974b2e9b\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:02:51.487715Z",
     "start_time": "2024-12-26T20:02:51.484272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'regression',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 10001},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 4000}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:02:55.002855Z",
     "start_time": "2024-12-26T20:02:54.968614Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/tabsyn_white_box/train/tabsyn_1/y_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# preprocess data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_num, X_cat, categories, d_numerical \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data/tabsyn_white_box/train/tabsyn_1/train_with_id.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data/tabsyn_white_box/train/tabsyn_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransforms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# separate train and test data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X_train_num, X_test_num \u001b[38;5;241m=\u001b[39m X_num\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/data.py:160\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(dataset_path, ref_dataset_path, transforms, task_type, inverse, concat)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m    152\u001b[0m     dataset_path,\n\u001b[1;32m    153\u001b[0m     ref_dataset_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     concat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ):\n\u001b[1;32m    159\u001b[0m     T \u001b[38;5;241m=\u001b[39m Transformations(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtransforms)\n\u001b[0;32m--> 160\u001b[0m     ref_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_dataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchange_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m make_dataset(\n\u001b[1;32m    169\u001b[0m         data_path\u001b[38;5;241m=\u001b[39mdataset_path,\n\u001b[1;32m    170\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         concat\u001b[38;5;241m=\u001b[39mconcat,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transforms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/data.py:636\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(data_path, T, task_type, change_val, concat)\u001b[0m\n\u001b[1;32m    633\u001b[0m y \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 636\u001b[0m     X_num_t, X_cat_t, y_t \u001b[38;5;241m=\u001b[39m \u001b[43mread_pure_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_num \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m concat:\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabSyn/src/data.py:581\u001b[0m, in \u001b[0;36mread_pure_data\u001b[0;34m(path, split)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_pure_data\u001b[39m(path, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 581\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     X_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     X_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/tabsyn_white_box/train/tabsyn_1/y_train.npy'"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(\n",
    "    \"../../data/tabsyn_white_box/train/tabsyn_1/train_with_id.csv\",\n",
    "    ref_dataset_path=\"../../data/tabsyn_white_box/train/tabsyn_1\",\n",
    "    transforms=raw_config[\"transforms\"],\n",
    "    task_type=raw_config[\"task_type\"],\n",
    ")\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = (\n",
    "    torch.tensor(X_train_num).float(),\n",
    "    torch.tensor(X_test_num).float(),\n",
    ")\n",
    "X_train_cat, X_test_cat = torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:48:07.176703Z",
     "start_time": "2024-12-26T20:48:07.038290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:54:28.978608Z",
     "start_time": "2024-12-26T20:54:28.174179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\r\n",
      "------------------------- --------------\r\n",
      "absl-py                   2.1.0\r\n",
      "aiohappyeyeballs          2.4.3\r\n",
      "aiohttp                   3.10.10\r\n",
      "aiosignal                 1.3.1\r\n",
      "anaconda-anon-usage       0.4.4\r\n",
      "anyio                     4.7.0\r\n",
      "appnope                   0.1.4\r\n",
      "archspec                  0.2.3\r\n",
      "argon2-cffi               23.1.0\r\n",
      "argon2-cffi-bindings      21.2.0\r\n",
      "arrow                     1.3.0\r\n",
      "asttokens                 3.0.0\r\n",
      "async-lru                 2.0.4\r\n",
      "attrs                     24.2.0\r\n",
      "babel                     2.16.0\r\n",
      "bandit                    1.7.10\r\n",
      "beautifulsoup4            4.12.3\r\n",
      "black                     24.10.0\r\n",
      "bleach                    6.2.0\r\n",
      "boltons                   23.0.0\r\n",
      "boto3                     1.35.65\r\n",
      "botocore                  1.35.65\r\n",
      "Brotli                    1.0.9\r\n",
      "certifi                   2024.8.30\r\n",
      "cffi                      1.16.0\r\n",
      "cfgv                      3.4.0\r\n",
      "charset-normalizer        2.0.4\r\n",
      "click                     8.1.7\r\n",
      "colorama                  0.4.6\r\n",
      "comm                      0.2.2\r\n",
      "conda                     24.9.2\r\n",
      "conda-content-trust       0.2.0\r\n",
      "conda-libmamba-solver     24.1.0\r\n",
      "conda-package-handling    2.2.0\r\n",
      "conda_package_streaming   0.9.0\r\n",
      "cryptography              42.0.5\r\n",
      "cssselect                 1.2.0\r\n",
      "d                         0.2.2\r\n",
      "debugpy                   1.8.11\r\n",
      "decaf_synthetic_data      0.1.6\r\n",
      "decorator                 5.1.1\r\n",
      "defusedxml                0.7.1\r\n",
      "disjoint_set              0.8.0\r\n",
      "distlib                   0.3.9\r\n",
      "distro                    1.9.0\r\n",
      "executing                 2.1.0\r\n",
      "fastjsonschema            2.21.1\r\n",
      "filelock                  3.16.1\r\n",
      "flake8                    7.1.1\r\n",
      "fqdn                      1.5.1\r\n",
      "frozendict                2.4.2\r\n",
      "frozenlist                1.4.1\r\n",
      "fsspec                    2024.9.0\r\n",
      "grpcio                    1.67.0\r\n",
      "h11                       0.14.0\r\n",
      "httpcore                  1.0.7\r\n",
      "httpx                     0.28.1\r\n",
      "icecream                  2.1.3\r\n",
      "identify                  2.6.1\r\n",
      "idna                      3.7\r\n",
      "iniconfig                 2.0.0\r\n",
      "ipykernel                 6.29.5\r\n",
      "ipython                   8.30.0\r\n",
      "ipywidgets                8.1.5\r\n",
      "isoduration               20.11.0\r\n",
      "jedi                      0.19.2\r\n",
      "Jinja2                    3.1.4\r\n",
      "jmespath                  1.0.1\r\n",
      "joblib                    1.4.2\r\n",
      "json5                     0.10.0\r\n",
      "jsonpatch                 1.33\r\n",
      "jsonpointer               2.1\r\n",
      "jsonschema                4.23.0\r\n",
      "jsonschema-specifications 2024.10.1\r\n",
      "jupyter                   1.1.1\r\n",
      "jupyter_client            8.6.3\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.11.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.14.2\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.3.4\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.3\r\n",
      "jupyterlab_widgets        3.0.13\r\n",
      "libmambapy                1.5.8\r\n",
      "lightgbm                  4.3.0\r\n",
      "lightning-utilities       0.11.8\r\n",
      "loguru                    0.7.2\r\n",
      "lxml                      5.3.0\r\n",
      "Markdown                  3.7\r\n",
      "markdown-it-py            3.0.0\r\n",
      "MarkupSafe                2.1.5\r\n",
      "matplotlib-inline         0.1.7\r\n",
      "matrix                    3.0.0\r\n",
      "maven                     0.0.6\r\n",
      "mccabe                    0.7.0\r\n",
      "mdurl                     0.1.2\r\n",
      "menuinst                  2.0.2\r\n",
      "mistune                   3.0.2\r\n",
      "mpmath                    1.3.0\r\n",
      "multidict                 6.1.0\r\n",
      "mypy-extensions           1.0.0\r\n",
      "nbclient                  0.10.2\r\n",
      "nbconvert                 7.16.4\r\n",
      "nbformat                  5.10.4\r\n",
      "nest-asyncio              1.6.0\r\n",
      "networkx                  2.8.8\r\n",
      "nodeenv                   1.9.1\r\n",
      "notebook                  7.3.1\r\n",
      "notebook_shim             0.2.4\r\n",
      "numpy                     1.26.4\r\n",
      "overrides                 7.7.0\r\n",
      "packaging                 23.2\r\n",
      "pandas                    2.2.3\r\n",
      "pandocfilters             1.5.1\r\n",
      "parso                     0.8.4\r\n",
      "pathspec                  0.12.1\r\n",
      "pbr                       6.1.0\r\n",
      "pexpect                   4.9.0\r\n",
      "pip                       24.0\r\n",
      "platformdirs              3.10.0\r\n",
      "pluggy                    1.5.0\r\n",
      "pre_commit                4.0.1\r\n",
      "prometheus_client         0.21.1\r\n",
      "prompt_toolkit            3.0.48\r\n",
      "propcache                 0.2.0\r\n",
      "protobuf                  4.25.3\r\n",
      "psutil                    5.9.0\r\n",
      "ptyprocess                0.7.0\r\n",
      "pure_eval                 0.2.3\r\n",
      "py4j                      0.10.9.7\r\n",
      "pycodestyle               2.12.1\r\n",
      "pycosat                   0.6.6\r\n",
      "pycparser                 2.21\r\n",
      "pyflakes                  3.2.0\r\n",
      "Pygments                  2.18.0\r\n",
      "pyquery                   2.0.1\r\n",
      "PySocks                   1.7.1\r\n",
      "pyspark                   3.5.3\r\n",
      "pytest                    8.3.3\r\n",
      "python-dateutil           2.9.0.post0\r\n",
      "python-json-logger        3.2.1\r\n",
      "pytorch-lightning         1.9.5\r\n",
      "pytz                      2024.2\r\n",
      "PyYAML                    6.0.2\r\n",
      "pyzmq                     26.2.0\r\n",
      "referencing               0.35.1\r\n",
      "requests                  2.31.0\r\n",
      "rfc3339-validator         0.1.4\r\n",
      "rfc3986-validator         0.1.1\r\n",
      "rich                      13.9.2\r\n",
      "rpds-py                   0.22.3\r\n",
      "ruamel.yaml               0.17.21\r\n",
      "s3transfer                0.10.3\r\n",
      "scikit-learn              1.5.2\r\n",
      "scipy                     1.13.1\r\n",
      "Send2Trash                1.8.3\r\n",
      "setuptools                69.5.1\r\n",
      "six                       1.16.0\r\n",
      "sniffio                   1.3.1\r\n",
      "soupsieve                 2.6\r\n",
      "stack-data                0.6.3\r\n",
      "stevedore                 5.3.0\r\n",
      "sympy                     1.13.3\r\n",
      "tensorboard               2.17.0\r\n",
      "tensorboard_data_server   0.7.0\r\n",
      "terminado                 0.18.1\r\n",
      "threadpoolctl             3.5.0\r\n",
      "tinycss2                  1.4.0\r\n",
      "torch                     2.2.2\r\n",
      "torchmetrics              1.5.0\r\n",
      "torchtext                 0.17.2\r\n",
      "tornado                   6.4.2\r\n",
      "tqdm                      4.66.2\r\n",
      "traitlets                 5.14.3\r\n",
      "truststore                0.8.0\r\n",
      "types-python-dateutil     2.9.0.20241206\r\n",
      "typing_extensions         4.12.2\r\n",
      "tzdata                    2024.2\r\n",
      "uri-template              1.3.0\r\n",
      "urllib3                   2.1.0\r\n",
      "virtualenv                20.27.0\r\n",
      "wcwidth                   0.2.13\r\n",
      "webcolors                 24.11.1\r\n",
      "webencodings              0.5.1\r\n",
      "websocket-client          1.8.0\r\n",
      "Werkzeug                  3.0.3\r\n",
      "wheel                     0.43.0\r\n",
      "widgetsnbextension        4.0.13\r\n",
      "xgboost                   2.1.1\r\n",
      "xlrd                      2.0.1\r\n",
      "yarl                      1.15.5\r\n",
      "zstandard                 0.22.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(\n",
    "    train_loader,\n",
    "    X_test_num,\n",
    "    X_test_cat,\n",
    "    num_numerical_features=d_numerical,\n",
    "    num_classes=categories,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(\n",
    "    **raw_config[\"model_params\"], optim_params=raw_config[\"train\"][\"optim\"][\"vae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}/vae\", exist_ok=True)\n",
    "tabsyn.train_vae(\n",
    "    **raw_config[\"loss_params\"],\n",
    "    num_epochs=raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "    save_path=os.path.join(MODEL_PATH, DATA_NAME, \"vae\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(\n",
    "    X_train_num, X_train_cat, vae_ckpt_dir=os.path.join(MODEL_PATH, DATA_NAME, \"vae\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(\n",
    "    os.path.join(MODEL_PATH, DATA_NAME, \"vae\")\n",
    ")  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "latent_train_data = (train_z - mean) / 2\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size=raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(\n",
    "    in_dim=train_z.shape[1],\n",
    "    hid_dim=train_z.shape[1],\n",
    "    optim_params=raw_config[\"train\"][\"optim\"][\"diffusion\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}\", exist_ok=True)\n",
    "# train diffusion model\n",
    "tabsyn.train_diffusion(\n",
    "    latent_train_loader,\n",
    "    num_epochs=raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "    ckpt_path=os.path.join(MODEL_PATH, DATA_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_embeddings_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\")\n",
    "pretrained_model_path = os.path.join(MODEL_PATH, DATA_NAME)\n",
    "\n",
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params=None)\n",
    "\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(\n",
    "    in_dim=train_z.shape[1], hid_dim=train_z.shape[1], optim_params=None\n",
    ")\n",
    "\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir=pretrained_model_path, dif_ckpt_name=\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(\n",
    "    os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "    ref_dataset_path=REF_DATA_PATH,\n",
    "    transforms=raw_config[\"transforms\"],\n",
    "    task_type=raw_config[\"task_type\"],\n",
    "    inverse=True,\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.join(SYNTH_DATA_DIR, DATA_NAME), exist_ok=True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1]\n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(\n",
    "    num_samples,\n",
    "    in_dim,\n",
    "    mean_input_emb,\n",
    "    info=data_info,\n",
    "    num_inverse=num_inverse,\n",
    "    cat_inverse=cat_inverse,\n",
    "    save_path=os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
