{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models\n",
    "\n",
    "Recent tabular data synthesis research has focused on single tables, while real-world applications often involve complex, interconnected tables. Existing methods for multi-relational data synthesis struggle with scalability and long-range dependencies. This paper introduces Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM), using clustering labels to model inter-table relationships, particularly foreign key constraints. ClavaDDPM efficiently propagates latent variables across tables, capturing long-range dependencies. Evaluations show ClavaDDPM outperforms existing methods on multi-table data and remains competitive for single-table data.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [ClavaDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Clustring]()\n",
    "    \n",
    "    4.3. [Model Training]()\n",
    "    \n",
    "    4.4. [Model Sampling]()\n",
    "    \n",
    "    \n",
    "    \n",
    "6. [Model Evaluation]()\n",
    "\n",
    "    6.1. [Multi-Table Metrics]()\n",
    "    \n",
    "    6.2. [Single-Table Metrics]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/golobs/Library/Caches/pypoetry/virtualenvs/midst-models-Xt6qy6Zf-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/golobs/Library/Caches/pypoetry/virtualenvs/midst-models-Xt6qy6Zf-py3.9/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplex_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     clava_clustering,\n\u001b[1;32m      5\u001b[0m     clava_training,\n\u001b[1;32m      6\u001b[0m     clava_load_pretrained,\n\u001b[1;32m      7\u001b[0m     clava_load_synthesized_data,\n\u001b[1;32m      8\u001b[0m     clava_load_synthesized_data_updated,\n\u001b[1;32m      9\u001b[0m     clava_synthesizing,\n\u001b[1;32m     10\u001b[0m     clava_eval,\n\u001b[1;32m     11\u001b[0m     load_configs,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_multi_table\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_multi_metadata\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/multi_table_ClavaDDPM/complex_pipeline.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msdv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiTableMetadata\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_multi_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_multi_report\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/multi_table_ClavaDDPM/gen_multi_report.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msdv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msingle_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_quality \u001b[38;5;28;01mas\u001b[39;00m single_eval_quality\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msdv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTableMetadata\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_multi_table\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidst_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_table_ClavaDDPM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     baseline_load_synthetic_data,\n\u001b[1;32m     15\u001b[0m     clava_load_synthetic_data,\n\u001b[1;32m     16\u001b[0m     get_multi_metadata,\n\u001b[1;32m     17\u001b[0m     sdv_load_synthetic_data,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_avg_long_range_scores\u001b[39m(res):\n",
      "File \u001b[0;32m~/PycharmProjects/MIDSTModels/midst_models/multi_table_ClavaDDPM/pipeline_utils.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.multi_table_ClavaDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_load_synthesized_data,\n",
    "    clava_load_synthesized_data_updated,\n",
    "    clava_synthesizing,\n",
    "    clava_eval,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.multi_table_ClavaDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.multi_table_ClavaDDPM.report_utils import get_multi_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify.\n",
    "\n",
    "A sample configuration file is available at `configs/berka.json`, where general parameters can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = \"configs/berka.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. We demonstrate the dataset's metadata and parent-child relationships to provide a clearer understanding of its structure. Following this, we perform clustering to preprocess the data, facilitating the training process for the ClavaDDPM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use all the tables from the Berka dataset. You can access the Berka dataset files for ClavaDDPM [here](https://drive.google.com/drive/folders/1rmJ_E6IzG25eCL3foYAb2jVmAstXktJ1?usp=drive_link). The BERKA dataset is a comprehensive banking dataset originally released by the Czech bank ČSOB for the Financial Modeling and Analysis (FMA) competition in 1999. It provides detailed financial data on transactions, accounts, loans, credit cards, and demographic information for thousands of customers over multiple years.\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. \n",
    "The following files are needed to be present in the data directory:\n",
    "- `{table}.csv`: The train susbet from the Berka dataset for all the tables.\n",
    "- `test.csv`: The transactions susbet from the Berka dataset used for evaluation.\n",
    "- `{table}_label_encoders.pkl`: The label encoders used to encode the table if you are using the already preprocessed data from the shared files.\n",
    "- `{table}_domain.json`: This file contains the domain information for each column in the transactions table. Sample domain files are available in `configs/domain_files`\n",
    "- `dataset_meta.json`: The configuration file defines the relationships between different tables in the dataset. A sample configuration file is available at `configs/dataset_meta.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-table dataset\n",
    "# In this step, we load the multi-table dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "# We organize the multi-table dataset as a dictionary of tables, a list of relation orders, and a dictionary of dataset metadata.\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])\n",
    "print(\"\")\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "print(list(tables.keys()))\n",
    "print(\"\")\n",
    "\n",
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for i, (from_item, to_item) in enumerate(relation_order):\n",
    "    print(\"Relation {}: {} ---> {}\".format(i, from_item, to_item))\n",
    "print(\"\")\n",
    "\n",
    "# Visualize the parent-child relationship within the multi-table dataset\n",
    "multi_meta = get_multi_metadata(tables, relation_order)\n",
    "multi_meta.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/user-attachments/assets/b190edb0-a16a-47b7-9534-cab46e40d0d0\" alt=\"ClavaDDPM Model Pipeline\" width=\"960\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the training process for the ClavaDDPM model. The diagram above, taken from the original paper, illustrates the main steps: \n",
    "\n",
    "(a) **Latent learning and table augmentation (steps 1-2):** This step crossponds to clustering section, where we aim to augmente each table with associated clustering labels that used to capture inter-table relationships.\n",
    "\n",
    "(b) **Training (steps 3-5):** This step corresponds to the model training section, where we train separate conditional diffusion models and the cluster classifier models on each augmented table.\n",
    "\n",
    "(c) **Synthesis (steps 6-8):** This step corresponds to the model sampling section, where we sample the table size and generate data based on the parent-child constraints (i.e., relation order).\n",
    "\n",
    "We will implement and demonstrate each section below, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "To get started, this paper first introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. Specifically, [Gaussian Process Latent Variable Models (GPLVM)](https://pyro.ai/examples/gplvm.html) are used to discover low-dimensional manifolds in noisy, high-dimensional spaces. We run the clustering algorithm below to preprocess the data for training the ClavaDDPM model. Additionally, we empirically determine the distribution of table sizes in the dataset, which will be used in the later sampling process.\n",
    "\n",
    "Mathematically, let $x$ and $y$ denote the child and parent tables, respectively. We consider $k$ clusters and model the distribution of $h = (x; \\lambda y)$ with a Gaussian distribution around its corresponding centroid $c$, i.e.,\n",
    "$$\n",
    "P(h) = \\sum_{c=1}^{k} P(c) P(h \\mid c) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(h; \\mu_c, \\Sigma_c),\n",
    "$$\n",
    "where the coefficient $\\lambda$ is the reweighting term called the parent scale. We opt for diagonal covariance, i.e., $\\Sigma_c = \\operatorname{diag}(\\ldots, \\sigma_l^2, \\ldots)$, which, when properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given the cluster. \n",
    "\n",
    "A summary of the important parameters for the clustering step includes:\n",
    "- `parent_scale`: reweighting coefficient $\\lambda$ for the parent table. The default value is 1.0. It is not a sensitive factor. Recommended range for tuning: 1.0 to 2.\n",
    "- `num_clusters`: the number of clustering centers $k$ for child tables. The default is 20. Too few or too many clusters may compromise performance. Recommended range for tuning: 10 to 50.\n",
    "- `clustering_method`: ClavaDDPM provides two ways to initialize GMM-based clustering. `gmm` uses `kmeans` for initialization, and the default method `both` uses `k-means++`. It is recommended to use `both`.\n",
    "\n",
    "We expect the clava_cluster function to return following outputs:\n",
    "- `tables`: containing the updated relational tables with data augmentation, where the latent variable is attached to the parent tables.\n",
    "- `all_group_lengths_prob_dicts`: which is a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs[\"clustering\"]\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "    tables, relation_order, save_dir, configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clustering results set, we can proceed to launch the training in PyTorch. As the dataset contains multiple tables, we train separate conditional diffusion models and cluster classifier models on each augmented table. Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"diffusion\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the relation order again, each line indicates one conditional generative model {}\".format(\n",
    "        \"=\" * 20, \"=\" * 20\n",
    "    )\n",
    ")\n",
    "for i, (from_item, to_item) in enumerate(relation_order):\n",
    "    print(\"Relation {}: {} ---> {}\".format(i, from_item, to_item))\n",
    "print(\"\")\n",
    "\n",
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-trained models\n",
    "## save_dir was determined when loading the config file\n",
    "models = clava_load_pretrained(relation_order, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "To assess the trained model's performance, we generate synthetic samples and showcase the results qualitatively. We initiate the generation process by sampling the table size (i.e., the number of rows per table) and performing conditional generation to meet the parent-child constraints (i.e., relation order). Quantitative evaluations will be conducted in the next section.\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling.\n",
    "- `classifier_scale` ($\\eta$): Controls the magnitude of classifier gradients during guided sampling, balancing sample quality and conditional sampling accuracy. The default value is 1.0. When $\\eta = 0$ (disabling classifier conditioning), single column densities (1-way) may improve but fail to capture long-range correlations. When $\\eta = 2$, the increased conditioning weight significantly improves the modeling of multi-hop correlations compared to $\\eta = 0$. Recommended tuning range: 0 to 2.\n",
    "\n",
    "<!-- We also conduct a matching process to determine the parent-child table relationship of the generated data. This process uses an approximate nearest neighbor search-based matching technique, providing a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents.\n",
    "\n",
    "Important parameters are as follows:\n",
    "- `num_matching_clusters`: Number of clusters used in the matching process.\n",
    "- `matching_batch_size`: Mini-batch size for table matching.\n",
    "- `unique_matching`: Boolean flag indicating if the matching result should be unique.\n",
    "- `no_matching`: Boolean flag to disable the matching process. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"sampling\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables,\n",
    "    relation_order,\n",
    "    save_dir,\n",
    "    all_group_lengths_prob_dicts,\n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == \"object\":\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing zero decimal values\n",
    "\n",
    "Since the categorical features might have zero floating-point values, we need to convert them back to integers for evaluation.\n",
    "The list of categorical columns is provided in the `congis/info_files/{table}_info.json` files. We convert the floating-point values back to integers and save the synthetic data in the same format as the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table in cleaned_tables.items():\n",
    "    info_path = f\"configs/info_files/{table_name}_info.json\"\n",
    "\n",
    "    with open(info_path, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "\n",
    "    cat_col_idx = info[\"cat_col_idx\"]\n",
    "    target_col_idx = info[\"target_col_idx\"]\n",
    "\n",
    "    task_type = info[\"task_type\"]\n",
    "    if task_type != \"regression\":\n",
    "        cat_col_idx += target_col_idx\n",
    "\n",
    "    for idx in cat_col_idx:\n",
    "        col = table.columns[idx]\n",
    "        table[col] = table[col].apply(\n",
    "            lambda x: int(float(x)) if str(x).replace(\".0\", \"\").isdigit() else x\n",
    "        )\n",
    "    table.to_csv(f\"{save_dir}/{table_name}_synthetic_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Table Metrics\n",
    "In this step, we quantitatively evaluate the generated tabular data by computing metrics to determine the accuracy of the predictions, specifically assessing how closely the generated data matches the observed samples in the reference dataset.\n",
    "\n",
    "In particular, the critical multi-table metrics are as follows:\n",
    "\n",
    "1. Pair-wise column correlation (k-hop): This metric measures the correlations between columns from tables at a distance k (e.g., 0-hop for columns within the same table, 1-hop for a column and a column from its parent or child table).\n",
    "\n",
    "2. Average 2-way: This metric computes the average of all k-hop column-pair correlations, taking into account both short-range (k = 0) and longer-range (k > 0) dependencies.\n",
    "\n",
    "We use the [SDV evaluation API](https://docs.sdv.dev/sdv/multi-table-data/evaluation/data-quality) to obtain these metrics. For more details about the computation process, refer to their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load existing synthetic data from the stored files you can run the commented code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load original synthesized data\n",
    "# cleaned_tables = clava_load_synthesized_data(tables.keys(), save_dir) \n",
    "# # Load updated synthesized data\n",
    "# cleaned_tables = clava_load_synthesized_data_updated(tables.keys(), save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-table Evaluation\n",
    "report = clava_eval(tables, save_dir, configs, relation_order, cleaned_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the multi-table metrics\n",
    "n_rows = 3\n",
    "for key, val in report.items():\n",
    "    if key in [\"hop_relation\", \"avg_scores\", \"all_avg_score\"]:\n",
    "        if key == \"hop_relation\":\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                if k > 0:\n",
    "                    print(\n",
    "                        \"{:20}-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\".format(\n",
    "                            k\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"{:20}-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\".format(\n",
    "                            k\n",
    "                        )\n",
    "                    )\n",
    "                for i_row, (k2, v2) in enumerate(v.items()):\n",
    "                    if i_row < n_rows:\n",
    "                        print(\"{:20} {}: {}\".format(\"\", k2, v2))\n",
    "                print(\"{:20} ...... other rows are omitted ......\\n\".format(\"\"))\n",
    "        elif key == \"avg_scores\":\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                print(\"{:20}-hop column correlation: {}\".format(k, v))\n",
    "        elif key == \"all_avg_score\":\n",
    "            print(\"{:20}: {}\".format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Table Metrics\n",
    "While here we study multi-table metrics, we also provide a notebook to evaluate single table metrics for each synthesized table. Please refer to `evalutate_synthetic_data.ipynb` for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (poetry-env)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
