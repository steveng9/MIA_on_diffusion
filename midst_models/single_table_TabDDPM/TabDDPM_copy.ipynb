{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABDDPM: Modelling Tabular Data with Diffusion Models\n",
    "\n",
    "Directly applying diffusion models to general tabular problems can be challenging because data points are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data complicates accurate modeling, as individual features can vary widely in nature; some may be continuous, while others are discrete. In this notebook, we explore **TabDDPM** â€” a diffusion model that can be universally applied to tabular datasets and effectively handles both categorical and numerical features.\n",
    "\n",
    "Our primary focus in this work is synthetic data generation, which is in high demand for many tabular tasks. Firstly, tabular datasets are often limited in size, unlike vision or NLP problems where large amounts of additional data are readily available online. Secondly, properly generated synthetic datasets do not contain actual user data, thus avoiding GDPR-like regulations and allowing for public sharing without compromising anonymity.\n",
    "\n",
    "In this notebook, we work with the ClavaDDPM implementation, which is originally designed for multi-table data synthesis. However, by applying a specific single-table configuration, we can effectively leverage it for single-table synthesis as well. This configuration activates TabDDPM, a component within ClavaDDPM tailored for single-table scenarios.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [TabDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Model Training]()\n",
    "    \n",
    "    4.3. [Model Sampling]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T04:25:26.803898Z",
     "start_time": "2024-12-27T04:25:26.634963Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomplex_pipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      4\u001B[0m     clava_clustering,\n\u001B[1;32m      5\u001B[0m     clava_training,\n\u001B[1;32m      6\u001B[0m     clava_load_pretrained,\n\u001B[1;32m      7\u001B[0m     clava_synthesizing,\n\u001B[1;32m      8\u001B[0m     load_configs,\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline_modules\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_multi_table\n",
      "File \u001B[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabDDPM/complex_pipeline.py:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msdv\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiTableMetadata\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgen_multi_report\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_multi_report\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline_modules\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabDDPM/gen_multi_report.py:12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msdv\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluate_quality \u001B[38;5;28;01mas\u001B[39;00m single_eval_quality\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msdv\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SingleTableMetadata\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_multi_table\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmidst_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msingle_table_TabDDPM\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreport_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     14\u001B[0m     baseline_load_synthetic_data,\n\u001B[1;32m     15\u001B[0m     clava_load_synthetic_data,\n\u001B[1;32m     16\u001B[0m     get_multi_metadata,\n\u001B[1;32m     17\u001B[0m     sdv_load_synthetic_data,\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_avg_long_range_scores\u001B[39m(res):\n",
      "File \u001B[0;32m~/PycharmProjects/MIDSTModels/midst_models/single_table_TabDDPM/pipeline_utils.py:5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfaiss\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'faiss'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T04:25:05.844819Z",
     "start_time": "2024-12-27T04:25:05.686345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!python --version\n",
    "# !python -m pip install ipykernel\n",
    "# !python -m ipykernel install --user\n",
    "# !cat /Users/soma/Library/Jupyter/kernels/python3/kernel.json"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify.\n",
    "\n",
    "A sample configuration file is available at `configs/trans.json`, where general parameters can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = \"configs/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this notebook, we use the Transactions table from the Berka dataset. You can access the Berka dataset files for TabDDPM [here](https://drive.google.com/drive/folders/1rmJ_E6IzG25eCL3foYAb2jVmAstXktJ1?usp=drive_link)\n",
    "The BERKA dataset is a comprehensive banking dataset originally released by the Czech bank ÄŒSOB for the Financial Modeling and Analysis (FMA) competition in 1999. It provides detailed financial data on transactions, accounts, loans, credit cards, and demographic information for thousands of customers over multiple years.\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. \n",
    "The following files are needed to be present in the data directory:\n",
    "- `train.csv`: The transactions susbet from the Berka dataset used for training. Note that the id columns (columns ending in \"_id\") should be removed from the training data.\n",
    "- `test.csv`: The transactions susbet from the Berka dataset used for evaluation. Note that the id columns (columns ending in \"_id\") should be removed from the test data.\n",
    "- `trans_label_encoders.pkl`: The label encoders used to encode the transactions table if you are using the already preprocessed data from shared files.\n",
    "- `trans_domain.json`: This file contains the domain information for each column in the transactions table. A sample domain file is available at `configs/trans_domain.json`\n",
    "- `dataset_meta.json`: The configuration file defines the relationships between different tables in the dataset. For single-table synthesis, it should be configured to include only one table. A sample configuration file is available at `configs/dataset_meta.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  dataset\n",
    "# In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])\n",
    "print(\"\")\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "print(list(tables.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabDDPM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabDDPM as well as its main hyperparameters loaded through config, which affect the modelâ€™s effectiveness. \n",
    "\n",
    "**TabDDPM:** uses the multinomial diffusion to model the categorical and binary features, and the Gaussian diffusion to model the numerical ones. The model is trained using the diffusion process, which is a continuous-time Markov chain that models the data distribution. In more detail, for a tabular data sample that consists of N numerical featuresand C categorical features with Ki categories each, TabDDPM takes one-hot encoded versions of categorical features as an input, and normalized numerical features. The figure below illustrates the diffusion process for classification problems; t, y and l denote a diffusion timestep, a class label, and logits, respectively.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/1b772284-de6a-44ad-8346-39b5f040cd31\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion models:**  are likelihood-based generative models that handle the data through forward and reverse Markov processes. The forward process gradually adds noise to an initial sample x0 from the data distribution q(x0) sampling noise from the predefined distributions q(xt|xtâˆ’1) with variances {Î²1, ..., Î²T}.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/6f610e06-ab5b-4974-97ce-9767baf254ea\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse diffusion proces gradually denoises a latent variable xTâˆ¼q(xT) and allows generating new data samples from q(x0). Distributions p(xtâˆ’1|xt) are usually unknown and approximated by a neural network with parameters Î¸.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/2c641eda-1678-4009-8d6e-88bf2ab24600\" width=\"280\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian diffusion models:** operate in continuous spaces where forward and reverse processes are characterized by Gaussian distributions:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/c0cfa4a8-9281-4a7a-aaaa-b220ffd05734\" width=\"330\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in general Î¸ parameters are learned from the data by optimizing a variational lower bound, in practice for Gaussian modeling, this objective can be simplified to the sum of mean-squared errors between ÎµÎ¸(xt ,t) and Îµ over all timesteps t as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/61f34373-3890-4785-98c6-6e103bd81950\" width=\"330\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial diffusion models:** are designed to generate categorical data where samples are a one-hot encoded categorical variable with K values. The multinomial forward diffusion process defines q(xt|xtâˆ’1) as a categorical distribution that corrupts the data by uniform noise over K classes: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/ced8bc14-9296-4a09-9881-64f90bed537d\" width=\"440\"/>\n",
    "</p>\n",
    "\n",
    "The reverse distribution pÎ¸(xtâˆ’1|xt) is parameterized as q(xtâˆ’1|xt,xË†0(xt,t)), where xË†0 is predicted by a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Note that ClavaDDPM introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. However in the single-table synthesis scenario, although we perform the clustering, it won't have an impact how the model is trained or sampled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs[\"clustering\"]\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "    tables, relation_order, save_dir, configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"diffusion\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-trained models\n",
    "## save_dir was determined when loading the config file\n",
    "models = clava_load_pretrained(relation_order, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"sampling\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables,\n",
    "    relation_order,\n",
    "    save_dir,\n",
    "    all_group_lengths_prob_dicts,\n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == \"object\":\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
